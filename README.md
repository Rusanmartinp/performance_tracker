# E-commerce Performance Tracker

An end-to-end analytics pipeline that simulates a real e-commerce advertising operation â€” from data generation to an interactive dashboard with forecasting, anomaly detection, and automated recommendations.

Built as a portfolio project to demonstrate data engineering, analysis, and visualization skills.

---

## Dashboard Preview

| Section | Description |
|---|---|
| KPI Cards | Revenue, ROAS, Clicks, Impressions for the latest day |
| Revenue Trend | Line chart by category over time |
| ARIMA Forecast | 30-day revenue forecast with 80% confidence interval, filterable by product/category |
| Anomaly Detection | Z-score based spike/drop detection over the last 7 days |
| Recommendations | Automated action table based on 7-day vs prior 7-day trends |

---

## Dashboard Preview

<p align="center">
  <img src="images/full_dashboard.png" width="900">
</p>

---

### ğŸ”¹ KPI Cards
<p align="center">
  <img src="images/kpi.png" width="700">
</p>

### ğŸ”¹ ARIMA Forecast
<p align="center">
  <img src="images/forecast.png" width="700">
</p>

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Simulated API     â”‚  FastAPI â€” generates realistic daily metrics
â”‚   (simulated_api/)  â”‚  per product with trend, seasonality & noise
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ETL Pipeline      â”‚  Fetches from API, applies category multipliers,
â”‚   (data_pipeline/)  â”‚  loads into PostgreSQL
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ SQL
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    PostgreSQL DB    â”‚  Stores products + daily_performance tables
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Analysis Modules   â”‚  forecasting / recommendation_engine /
â”‚  (analysis/)        â”‚  anomaly_detection / kpi_analysis
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit Dashboardâ”‚  Interactive UI with filters, charts, tables
â”‚  (dashboard/)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Tech Stack

| Layer | Technology |
|---|---|
| API | FastAPI |
| Database | PostgreSQL |
| ORM / Queries | SQLAlchemy, pandas |
| Forecasting | ARIMA via pmdarima (auto parameter selection) |
| Dashboard | Streamlit + Plotly |
| Containerization | Docker + Docker Compose |
| Testing | pytest |

---

## Quick Start (Docker â€” recommended)

**Prerequisites:** Docker and Docker Compose installed.

```bash
git clone https://github.com/youruser/performance-tracker.git
cd performance-tracker

# Copy and configure environment variables
cp .env.example .env

# Start all services (DB + API + Dashboard)
docker-compose up --build
```

Then open **http://localhost:8501** in your browser.

On first run, the ETL loads 90 days of simulated historical data automatically.

---

## Manual Setup (without Docker)

### 1. Create and activate virtual environment

```bash
python -m venv venv
source venv/bin/activate        # Linux/Mac
venv\Scripts\activate           # Windows
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure environment variables

```bash
cp .env.example .env
# Edit .env with your PostgreSQL credentials
```

### 4. Create the database tables

```sql
CREATE TABLE products (
    id       SERIAL PRIMARY KEY,
    name     TEXT NOT NULL,
    price    NUMERIC(10,2),
    category TEXT
);

CREATE TABLE daily_performance (
    date        DATE,
    product_id  INTEGER REFERENCES products(id),
    impressions INTEGER,
    clicks      INTEGER,
    ad_spend    NUMERIC(10,2),
    units_sold  INTEGER,
    revenue     NUMERIC(10,2),
    PRIMARY KEY (date, product_id)
);
```

### 5. Start the simulated API

```bash
uvicorn simulated_api.main:app --reload --port 8000
```

### 6. Run the ETL pipeline

```bash
python data_pipeline/etl.py
```

### 7. Launch the dashboard

```bash
streamlit run dashboard/app.py
```

---

## ğŸ§ª Running Tests

```bash
pytest tests/ -v
```

---

## Project Structure

```
performance_tracker/
â”œâ”€â”€ simulated_api/
â”‚   â””â”€â”€ main.py                   # FastAPI app â€” generates daily metrics per product
â”œâ”€â”€ data_pipeline/
â”‚   â””â”€â”€ etl.py                    # Extract from API, transform, load to PostgreSQL
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ forecasting.py            # ARIMA revenue forecast (auto parameter selection)
â”‚   â”œâ”€â”€ recommendation_engine.py  # 7-day trend comparison + action suggestions
â”‚   â”œâ”€â”€ anomaly_detection.py      # Z-score anomaly detection with hypothesis
â”‚   â”œâ”€â”€ kpi_analysis.py           # KPI calculations
â”‚   â””â”€â”€ promotion_analysis.py     # Promotion impact analysis
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                    # Streamlit dashboard
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_recommendation_engine.py
â”‚   â”œâ”€â”€ test_anomaly_detection.py
â”‚   â””â”€â”€ test_forecasting.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## Environment Variables

Copy `.env.example` to `.env` and fill in your values:

```
DB_USER=your_db_user
DB_PASSWORD=your_db_password
DB_HOST=localhost
DB_NAME=performance_tracker
```

When running with Docker, `DB_HOST` should be `db` (the service name).

---

## Key Design Decisions

**Why a simulated API instead of a CSV?**
Simulating a real HTTP data source makes the ETL more representative of production pipelines, where data comes from external ad platforms (Google Ads, Amazon, Meta).

**Why ARIMA over Prophet?**
Prophet is powerful but heavy. ARIMA with `auto_arima` selects optimal parameters automatically and is faster for daily revenue series with clear weekly seasonality.

**Why Z-score for anomaly detection?**
Simple, interpretable, and effective for univariate time series without requiring labeled training data. The threshold is configurable.
